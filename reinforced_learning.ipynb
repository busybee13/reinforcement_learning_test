{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "from PIL import ImageGrab\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"chrome://dino\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(200, 300)\n",
    "        try:\n",
    "            self._driver.get(game_url)\n",
    "        except Exception as e:\n",
    "            print('Exception', e)\n",
    "        #modifying game before training\n",
    "        if custom_config:\n",
    "            self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "        \n",
    "        time.sleep(0.25)# no actions are possible \n",
    "                        # for 0.25 sec after game starts, \n",
    "                        # skip learning at this time and make the model wait\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "        time.sleep(.5) # no action can be performed for the first time when game starts\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get_state(): accepts an array of actions, \n",
    "             performs the action on the agent \n",
    "returns :  new state, reward and if the game ended.\n",
    "'''\n",
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "    def get_state(self,actions):\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1*score/10 # dynamic reward calculation\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1: #else do nothing\n",
    "            self._agent.jump()\n",
    "            reward = 0.1*score/11\n",
    "        image = grab_screen() \n",
    "        \n",
    "        if self._agent.is_crashed():\n",
    "            self._game.restart()\n",
    "            reward = -11/score\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_screen(_driver = None):\n",
    "    #bbox = region of interest on the entire screen\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(40,180,440,400))) \n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "    \n",
    "def process_img(image):\n",
    "    #game is already in grey scale canvas, canny to get only edges and reduce unwanted objects(clouds)\n",
    "    # resale image dimensions\n",
    "    image = cv2.resize(image, (0,0), fx = 0.15, fy = 0.10) \n",
    "    #crop out the dino agent from the frame\n",
    "    image = image[2:38,10:50] #img[y:y+h, x:x+w] \n",
    "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200) #apply the canny edge detection\n",
    "    return  image    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyper parameters\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 40,20\n",
    "img_channels = 4 #We stack 4 frames\n",
    "ACTIONS = 2\n",
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same',input_shape=(img_cols,img_rows,img_channels)))  #20*40*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state):\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque() #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2).reshape(1,20,40,4) # stack 4 images to create placeholder input reshaped 1*20*40*4 \n",
    "    \n",
    "    OBSERVE = OBSERVATION\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at t\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if  random.random() <= epsilon: #randomly explore an action\n",
    "            print(\"----------Random Action----------\")\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else: # predict the output\n",
    "            q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "            max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "            action_index = max_Q \n",
    "            a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft() \n",
    "        \n",
    "        #only train if done observing; sample a minibatch to train on\n",
    "        if t > OBSERVE:\n",
    "            minibatch= random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "            loss = 0\n",
    "\n",
    "            for i in range(0, len(minibatch)):                \n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                inputs[i:i + 1] = state_t    \n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                print('before')\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                print(f'after, {Q_sa}')\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "                loss += model.train_on_batch(inputs, targets)\n",
    "        s_t = s_t1 \n",
    "        t = t + 1\n",
    "        print(\"TIMESTEP\", t, \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,\"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBatch(model, minibatch, s_t):\n",
    "  inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "  targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "  loss = 0\n",
    "  \n",
    "  for i in range(0, len(minibatch)):                \n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                inputs[i:i + 1] = state_t    \n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                print('before')\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                print(f'after, {Q_sa}')\n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "                loss += model.train_on_batch(inputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argument: observe, only plays if true, else trains\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)\n",
    "    model = buildmodel()\n",
    "    trainNetwork(model,game_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-42a0ecbc79b0>:17: DeprecationWarning: use options instead of chrome_options\n",
      "  self._driver = webdriver.Chrome(chrome_options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Message: unknown error: net::ERR_INTERNET_DISCONNECTED\n",
      "  (Session info: chrome=87.0.4280.141)\n",
      "\n",
      "Now we build the model\n",
      "We finish building the model\n",
      "TIMESTEP 1 / EPSILON 0.1 / ACTION 0 / REWARD 0.02 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 2 / EPSILON 0.1 / ACTION 0 / REWARD 0.030000000000000006 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 3 / EPSILON 0.1 / ACTION 0 / REWARD 0.04 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 4 / EPSILON 0.1 / ACTION 0 / REWARD 0.05 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 5 / EPSILON 0.1 / ACTION 0 / REWARD 0.06000000000000001 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 6 / EPSILON 0.1 / ACTION 0 / REWARD 0.07 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 7 / EPSILON 0.1 / ACTION 0 / REWARD 0.08 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 8 / EPSILON 0.1 / ACTION 0 / REWARD 0.09 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 9 / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 10 / EPSILON 0.1 / ACTION 0 / REWARD 0.11000000000000001 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 11 / EPSILON 0.1 / ACTION 0 / REWARD 0.12000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 12 / EPSILON 0.1 / ACTION 0 / REWARD 0.13 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 13 / EPSILON 0.1 / ACTION 0 / REWARD 0.13 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 14 / EPSILON 0.1 / ACTION 0 / REWARD 0.14 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 15 / EPSILON 0.1 / ACTION 0 / REWARD 0.15 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 16 / EPSILON 0.1 / ACTION 0 / REWARD 0.16 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 17 / EPSILON 0.1 / ACTION 0 / REWARD 0.17 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 18 / EPSILON 0.1 / ACTION 0 / REWARD 0.18 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 19 / EPSILON 0.1 / ACTION 0 / REWARD 0.19 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 20 / EPSILON 0.1 / ACTION 0 / REWARD 0.2 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 21 / EPSILON 0.1 / ACTION 0 / REWARD 0.21000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 22 / EPSILON 0.1 / ACTION 0 / REWARD 0.22000000000000003 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 23 / EPSILON 0.1 / ACTION 0 / REWARD 0.23000000000000004 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 24 / EPSILON 0.1 / ACTION 1 / REWARD 0.21818181818181823 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 25 / EPSILON 0.1 / ACTION 0 / REWARD 0.25 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 26 / EPSILON 0.1 / ACTION 0 / REWARD 0.26 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 27 / EPSILON 0.1 / ACTION 0 / REWARD 0.27 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 28 / EPSILON 0.1 / ACTION 0 / REWARD 0.28 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 29 / EPSILON 0.1 / ACTION 0 / REWARD 0.28 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 30 / EPSILON 0.1 / ACTION 0 / REWARD 0.3 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 31 / EPSILON 0.1 / ACTION 0 / REWARD 0.3 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 32 / EPSILON 0.1 / ACTION 0 / REWARD 0.31 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 33 / EPSILON 0.1 / ACTION 0 / REWARD 0.31 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 34 / EPSILON 0.1 / ACTION 0 / REWARD 0.32 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 35 / EPSILON 0.1 / ACTION 0 / REWARD 0.33 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 36 / EPSILON 0.1 / ACTION 0 / REWARD 0.34 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 37 / EPSILON 0.1 / ACTION 0 / REWARD 0.35 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 38 / EPSILON 0.1 / ACTION 0 / REWARD 0.36 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 39 / EPSILON 0.1 / ACTION 0 / REWARD -0.2894736842105263 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 40 / EPSILON 0.1 / ACTION 1 / REWARD 0.018181818181818184 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 41 / EPSILON 0.1 / ACTION 1 / REWARD 0.02727272727272728 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 42 / EPSILON 0.1 / ACTION 1 / REWARD 0.045454545454545456 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 43 / EPSILON 0.1 / ACTION 1 / REWARD 0.05454545454545456 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 44 / EPSILON 0.1 / ACTION 0 / REWARD 0.07 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 45 / EPSILON 0.1 / ACTION 0 / REWARD 0.08 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 46 / EPSILON 0.1 / ACTION 0 / REWARD 0.09 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 47 / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 48 / EPSILON 0.1 / ACTION 0 / REWARD 0.11000000000000001 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 49 / EPSILON 0.1 / ACTION 0 / REWARD 0.11000000000000001 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 50 / EPSILON 0.1 / ACTION 0 / REWARD 0.12000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 51 / EPSILON 0.1 / ACTION 0 / REWARD 0.13 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 52 / EPSILON 0.1 / ACTION 0 / REWARD 0.14 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 53 / EPSILON 0.1 / ACTION 0 / REWARD 0.15 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 54 / EPSILON 0.1 / ACTION 0 / REWARD 0.16 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 55 / EPSILON 0.1 / ACTION 0 / REWARD 0.17 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 56 / EPSILON 0.1 / ACTION 0 / REWARD 0.18 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 57 / EPSILON 0.1 / ACTION 0 / REWARD 0.19 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 58 / EPSILON 0.1 / ACTION 0 / REWARD 0.2 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 59 / EPSILON 0.1 / ACTION 0 / REWARD 0.21000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 60 / EPSILON 0.1 / ACTION 0 / REWARD 0.21000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 61 / EPSILON 0.1 / ACTION 0 / REWARD 0.23000000000000004 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 62 / EPSILON 0.1 / ACTION 0 / REWARD 0.23000000000000004 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 63 / EPSILON 0.1 / ACTION 0 / REWARD 0.24000000000000005 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 64 / EPSILON 0.1 / ACTION 0 / REWARD 0.25 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 65 / EPSILON 0.1 / ACTION 0 / REWARD 0.26 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 66 / EPSILON 0.1 / ACTION 0 / REWARD 0.26 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 67 / EPSILON 0.1 / ACTION 0 / REWARD 0.27 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 68 / EPSILON 0.1 / ACTION 0 / REWARD 0.28 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 69 / EPSILON 0.1 / ACTION 0 / REWARD 0.29000000000000004 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 70 / EPSILON 0.1 / ACTION 0 / REWARD 0.3 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 71 / EPSILON 0.1 / ACTION 0 / REWARD 0.31 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 72 / EPSILON 0.1 / ACTION 0 / REWARD 0.32 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 73 / EPSILON 0.1 / ACTION 0 / REWARD 0.33 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 74 / EPSILON 0.1 / ACTION 0 / REWARD -0.3333333333333333 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 75 / EPSILON 0.1 / ACTION 0 / REWARD 0.030000000000000006 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 76 / EPSILON 0.1 / ACTION 0 / REWARD 0.030000000000000006 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 77 / EPSILON 0.1 / ACTION 0 / REWARD 0.04 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 78 / EPSILON 0.1 / ACTION 0 / REWARD 0.05 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 79 / EPSILON 0.1 / ACTION 0 / REWARD 0.06000000000000001 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 80 / EPSILON 0.1 / ACTION 0 / REWARD 0.07 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 81 / EPSILON 0.1 / ACTION 0 / REWARD 0.09 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 82 / EPSILON 0.1 / ACTION 0 / REWARD 0.09 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 83 / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 84 / EPSILON 0.1 / ACTION 0 / REWARD 0.11000000000000001 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 85 / EPSILON 0.1 / ACTION 0 / REWARD 0.12000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 86 / EPSILON 0.1 / ACTION 0 / REWARD 0.13 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 87 / EPSILON 0.1 / ACTION 0 / REWARD 0.14 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 88 / EPSILON 0.1 / ACTION 0 / REWARD 0.14 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 89 / EPSILON 0.1 / ACTION 0 / REWARD 0.15 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 90 / EPSILON 0.1 / ACTION 0 / REWARD 0.16 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 91 / EPSILON 0.1 / ACTION 0 / REWARD 0.17 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 92 / EPSILON 0.1 / ACTION 0 / REWARD 0.18 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 93 / EPSILON 0.1 / ACTION 0 / REWARD 0.19 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 94 / EPSILON 0.1 / ACTION 0 / REWARD 0.2 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 95 / EPSILON 0.1 / ACTION 0 / REWARD 0.21000000000000002 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 96 / EPSILON 0.1 / ACTION 0 / REWARD 0.22000000000000003 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 97 / EPSILON 0.1 / ACTION 0 / REWARD 0.23000000000000004 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 98 / EPSILON 0.1 / ACTION 0 / REWARD 0.24000000000000005 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 99 / EPSILON 0.1 / ACTION 0 / REWARD 0.25 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "TIMESTEP 100 / EPSILON 0.1 / ACTION 1 / REWARD 0.22727272727272727 / Q_MAX  0 / Loss  0\n",
      "TIMESTEP 101 / EPSILON 0.1 / ACTION 0 / REWARD 0.26 / Q_MAX  0 / Loss  0\n",
      "before\n",
      "after, [[0. 0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "after, [[9.9922225e-05 0.0000000e+00]]\n",
      "before\n",
      "after, [[8.36062e-05 0.00000e+00]]\n",
      "before\n",
      "after, [[0.00011216 0.        ]]\n",
      "before\n",
      "after, [[0.00017404 0.        ]]\n",
      "before\n",
      "after, [[0.00024959 0.        ]]\n",
      "before\n",
      "after, [[-0.16129029 -0.2754026 ]]\n",
      "before\n",
      "after, [[ 8.4963453e-04 -3.0815212e-05]]\n",
      "before\n",
      "after, [[ 0.41985878 -0.23679335]]\n",
      "before\n",
      "after, [[ 0.24536122 -0.33099884]]\n",
      "before\n",
      "after, [[ 4.0662563e-03 -7.2413386e-06]]\n",
      "before\n",
      "after, [[ 0.46239975 -0.4319093 ]]\n",
      "before\n",
      "after, [[ 0.37802893 -0.29137233]]\n",
      "before\n",
      "after, [[ 0.5725067 -0.254966 ]]\n",
      "before\n",
      "after, [[ 0.53857464 -0.3850529 ]]\n",
      "before\n",
      "after, [[1.1308606e-02 2.8446004e-05]]\n",
      "before\n",
      "after, [[ 0.60654783 -0.35451752]]\n",
      "before\n",
      "after, [[ 0.50809914 -0.33519292]]\n",
      "before\n",
      "after, [[1.6568828e-02 7.1171322e-05]]\n",
      "before\n",
      "after, [[ 0.54915583 -0.36667648]]\n",
      "before\n",
      "after, [[ 2.0303501e-02 -4.8828962e-05]]\n",
      "before\n",
      "after, [[ 0.66288006 -0.29229027]]\n",
      "before\n",
      "after, [[ 0.5961643 -0.3618058]]\n",
      "before\n",
      "after, [[ 0.02685984 -0.00010622]]\n",
      "before\n",
      "after, [[ 0.65251976 -0.34073463]]\n",
      "before\n",
      "after, [[ 0.6386592  -0.31947008]]\n",
      "before\n",
      "after, [[ 0.7120209  -0.34914508]]\n",
      "before\n",
      "after, [[ 0.6317089  -0.25539616]]\n",
      "before\n",
      "after, [[ 0.79127824 -0.4796951 ]]\n",
      "before\n",
      "after, [[-0.16947344 -0.0585252 ]]\n",
      "before\n",
      "after, [[ 0.68049246 -0.275088  ]]\n",
      "before\n",
      "after, [[ 0.7933253  -0.46531302]]\n",
      "TIMESTEP 102 / EPSILON 0.099001 / ACTION 0 / REWARD 0.27 / Q_MAX  0.7933253 / Loss  0.3877713793772273\n",
      "before\n",
      "after, [[ 0.7351212  -0.33248106]]\n",
      "before\n",
      "after, [[ 0.712995   -0.20482293]]\n",
      "before\n",
      "after, [[0.05615461 0.00105737]]\n",
      "before\n",
      "after, [[ 0.8521783  -0.02693342]]\n",
      "before\n",
      "after, [[ 0.9785067  -0.02302822]]\n",
      "before\n",
      "after, [[ 1.0874026  -0.08746449]]\n",
      "before\n",
      "after, [[0.05422645 0.00044801]]\n",
      "before\n",
      "after, [[ 1.0945165  -0.25687343]]\n",
      "before\n",
      "after, [[ 1.0503645  -0.23969461]]\n",
      "before\n",
      "after, [[0.04911015 0.00013049]]\n",
      "before\n",
      "after, [[ 1.1239036  -0.07691284]]\n",
      "before\n",
      "after, [[ 1.1283859  -0.10729062]]\n",
      "before\n",
      "after, [[ 0.0442479  -0.00029086]]\n",
      "before\n",
      "after, [[ 1.1432526  -0.21225184]]\n",
      "before\n",
      "after, [[ 1.1737262  -0.15522821]]\n",
      "before\n",
      "after, [[ 0.04046936 -0.00044429]]\n",
      "before\n",
      "after, [[ 0.03971675 -0.00044343]]\n",
      "before\n",
      "after, [[ 0.03968717 -0.00044053]]\n",
      "before\n",
      "after, [[ 0.03989435 -0.00048316]]\n",
      "before\n",
      "after, [[ 0.04013073 -0.00052863]]\n",
      "before\n",
      "after, [[ 0.8989466  -0.18365422]]\n",
      "before\n",
      "after, [[ 0.9500836  -0.09746423]]\n",
      "before\n",
      "after, [[ 0.04361776 -0.0002503 ]]\n",
      "before\n",
      "after, [[ 0.04547907 -0.00033182]]\n",
      "before\n",
      "after, [[ 0.97637004 -0.22914545]]\n",
      "before\n",
      "after, [[ 0.95885247 -0.12358618]]\n",
      "before\n",
      "after, [[0.83245295 0.32256818]]\n",
      "before\n",
      "after, [[ 1.0949017  -0.14840272]]\n",
      "before\n",
      "after, [[ 1.0271465  -0.20407417]]\n",
      "before\n",
      "after, [[0.06345949 0.00105823]]\n",
      "before\n",
      "after, [[0.06764574 0.00162593]]\n",
      "before\n",
      "after, [[ 1.0632801  -0.19412571]]\n",
      "TIMESTEP 103 / EPSILON 0.098002 / ACTION 0 / REWARD -0.3235294117647059 / Q_MAX  1.0632801 / Loss  0.9365752179874107\n",
      "before\n",
      "after, [[0.07685786 0.00234168]]\n",
      "before\n",
      "after, [[0.08024614 0.00260447]]\n",
      "before\n",
      "after, [[ 1.0337012  -0.22385587]]\n",
      "before\n",
      "after, [[ 1.0567161  -0.22886656]]\n",
      "before\n",
      "after, [[ 1.1017784  -0.23133025]]\n",
      "before\n",
      "after, [[ 1.2028185  -0.22787824]]\n",
      "before\n",
      "after, [[ 1.3269757  -0.22205661]]\n",
      "before\n",
      "after, [[0.08119763 0.00283571]]\n",
      "before\n",
      "after, [[0.0790887  0.00291121]]\n",
      "before\n",
      "after, [[ 1.254625   -0.14093915]]\n",
      "before\n",
      "after, [[0.07411446 0.0031036 ]]\n",
      "before\n",
      "after, [[0.07184097 0.00321532]]\n",
      "before\n",
      "after, [[1.2285929  0.23413412]]\n",
      "before\n",
      "after, [[0.06819888 0.00233119]]\n",
      "before\n",
      "after, [[0.06674577 0.00176802]]\n",
      "before\n",
      "after, [[0.06553701 0.00149466]]\n",
      "before\n",
      "after, [[1.3011982  0.17071448]]\n",
      "before\n",
      "after, [[1.2604257  0.29101938]]\n",
      "before\n",
      "after, [[1.3107057  0.25563008]]\n",
      "before\n",
      "after, [[0.06435927 0.00070639]]\n",
      "before\n",
      "after, [[0.06491973 0.00053579]]\n",
      "before\n",
      "after, [[0.06626878 0.00062603]]\n",
      "before\n",
      "after, [[0.06818418 0.00066555]]\n",
      "before\n",
      "after, [[1.2301098  0.19116347]]\n",
      "before\n",
      "after, [[0.9418987  0.06872904]]\n",
      "before\n",
      "after, [[0.80557126 0.10370223]]\n",
      "before\n",
      "after, [[0.07824473 0.00057447]]\n",
      "before\n",
      "after, [[1.1988     0.20614405]]\n",
      "before\n",
      "after, [[1.1429387  0.09961802]]\n",
      "before\n",
      "after, [[0.9619729  0.09639677]]\n",
      "before\n",
      "after, [[0.9921111  0.19565983]]\n",
      "before\n",
      "after, [[0.10041266 0.00058591]]\n",
      "TIMESTEP 104 / EPSILON 0.097003 / ACTION 0 / REWARD -0.3235294117647059 / Q_MAX  0.10041266 / Loss  2.0927191697992384\n",
      "before\n",
      "after, [[0.10613082 0.00046673]]\n",
      "before\n",
      "after, [[0.11004835 0.00030866]]\n",
      "before\n",
      "after, [[ 1.1606252  -0.03692297]]\n",
      "before\n",
      "after, [[ 1.1699847 -0.0996072]]\n",
      "before\n",
      "after, [[ 0.11301475 -0.00012159]]\n",
      "before\n",
      "after, [[ 0.1118696  -0.00021889]]\n",
      "before\n",
      "after, [[ 1.3556905  -0.16843508]]\n",
      "before\n",
      "after, [[ 1.4213141  -0.13894482]]\n",
      "before\n",
      "after, [[ 1.4823745  -0.09815869]]\n",
      "before\n",
      "after, [[1.2981263 0.9556268]]\n",
      "before\n",
      "after, [[ 1.5022818  -0.06643169]]\n",
      "before\n",
      "after, [[ 1.4650059 -0.0898932]]\n",
      "before\n",
      "after, [[ 1.4408717  -0.11883567]]\n",
      "before\n",
      "after, [[ 1.4594253  -0.12653206]]\n",
      "before\n",
      "after, [[ 0.08519903 -0.00046517]]\n",
      "before\n",
      "after, [[ 0.08231692 -0.00028423]]\n",
      "before\n",
      "after, [[1.4884616  0.22794773]]\n",
      "before\n",
      "after, [[1.2684247  0.02157059]]\n",
      "before\n",
      "after, [[ 0.07471973 -0.00119547]]\n",
      "before\n",
      "after, [[ 0.07329743 -0.00095818]]\n",
      "before\n",
      "after, [[1.4038187 0.2297741]]\n",
      "before\n",
      "after, [[1.4814526 0.1971276]]\n",
      "before\n",
      "after, [[ 0.07075966 -0.00075702]]\n",
      "before\n",
      "after, [[ 0.0701855  -0.00073441]]\n",
      "before\n",
      "after, [[1.2723575  0.12050646]]\n",
      "before\n",
      "after, [[ 0.07080857 -0.00029639]]\n",
      "before\n",
      "after, [[1.485598   0.13871478]]\n",
      "before\n",
      "after, [[1.3924824  0.06235823]]\n",
      "before\n",
      "after, [[1.3180549  0.08863448]]\n",
      "before\n",
      "after, [[1.3884763  0.15442114]]\n",
      "before\n",
      "after, [[1.4912819  0.13895889]]\n",
      "before\n",
      "after, [[1.4480808 0.0927359]]\n",
      "TIMESTEP 105 / EPSILON 0.096004 / ACTION 0 / REWARD -0.3235294117647059 / Q_MAX  1.4480808 / Loss  1.9523316202685237\n",
      "before\n",
      "after, [[1.3734944  0.10990778]]\n",
      "before\n",
      "after, [[1.3093735  0.12518547]]\n",
      "before\n",
      "after, [[1.2847794  0.13515262]]\n",
      "before\n",
      "after, [[0.08362655 0.00021283]]\n",
      "before\n",
      "after, [[0.08337883 0.00022447]]\n",
      "before\n",
      "after, [[0.08277366 0.00021854]]\n",
      "before\n",
      "after, [[0.08187976 0.0002034 ]]\n",
      "before\n",
      "after, [[1.5903188  0.11222171]]\n",
      "before\n",
      "after, [[1.3982317  0.12835327]]\n",
      "before\n",
      "after, [[1.0988307 0.1473347]]\n",
      "before\n",
      "after, [[0.8664791  0.14829035]]\n",
      "before\n",
      "after, [[0.83790416 0.12310158]]\n",
      "before\n",
      "after, [[1.007389   0.09187707]]\n",
      "before\n",
      "after, [[ 7.166685e-02 -6.616916e-05]]\n",
      "before\n",
      "after, [[1.0510234  0.14100595]]\n",
      "before\n",
      "after, [[0.9225638  0.16715474]]\n",
      "before\n",
      "after, [[ 0.0677717  -0.00011906]]\n",
      "before\n",
      "after, [[0.9236945  0.09737653]]\n",
      "before\n",
      "after, [[1.0412396 0.0921404]]\n",
      "before\n",
      "after, [[1.099678  0.1366997]]\n",
      "before\n",
      "after, [[ 0.06528393 -0.00010886]]\n",
      "before\n",
      "after, [[0.96415555 0.13262501]]\n",
      "before\n",
      "after, [[0.965428   0.08617473]]\n",
      "before\n",
      "after, [[1.0431628  0.10451142]]\n",
      "before\n",
      "after, [[1.0993687  0.15516268]]\n",
      "before\n",
      "after, [[1.0798451  0.14413717]]\n",
      "before\n",
      "after, [[ 0.06607091 -0.00013362]]\n",
      "before\n",
      "after, [[ 6.6788428e-02 -2.5863337e-06]]\n",
      "before\n",
      "after, [[1.0805947  0.15030925]]\n",
      "before\n",
      "after, [[1.1069342  0.13515311]]\n",
      "before\n",
      "after, [[0.06956901 0.00076044]]\n",
      "before\n",
      "after, [[0.07068581 0.00116664]]\n",
      "TIMESTEP 106 / EPSILON 0.095005 / ACTION 0 / REWARD -0.3235294117647059 / Q_MAX  0.07068581 / Loss  1.9637747923843563\n",
      "before\n",
      "after, [[1.0826381  0.14137474]]\n",
      "before\n",
      "after, [[1.1302798  0.15112846]]\n",
      "before\n",
      "after, [[1.2133353  0.15600018]]\n",
      "before\n",
      "after, [[0.07334232 0.00256199]]\n",
      "before\n",
      "after, [[1.4481156  0.15170732]]\n",
      "before\n",
      "after, [[1.528713   0.18612099]]\n",
      "before\n",
      "after, [[1.5695723 0.2362708]]\n",
      "before\n",
      "after, [[1.5873929  0.26539797]]\n",
      "before\n",
      "after, [[0.06818555 0.00341862]]\n",
      "before\n",
      "after, [[0.06647559 0.00336053]]\n",
      "before\n",
      "after, [[0.06472299 0.00325683]]\n",
      "before\n",
      "after, [[0.06302477 0.00316695]]\n",
      "before\n",
      "after, [[1.5332435  0.12897241]]\n",
      "before\n",
      "after, [[0.05990614 0.00308396]]\n",
      "before\n",
      "after, [[1.6400036  0.19744802]]\n",
      "before\n",
      "after, [[0.05723519 0.00287786]]\n",
      "before\n",
      "after, [[0.05606493 0.00270123]]\n",
      "before\n",
      "after, [[0.05505231 0.00252822]]\n",
      "before\n",
      "after, [[1.5859277  0.18064895]]\n",
      "before\n",
      "after, [[0.05354823 0.00227685]]\n",
      "before\n",
      "after, [[1.2515881  0.21880272]]\n",
      "before\n",
      "after, [[1.3052304  0.17839713]]\n",
      "before\n",
      "after, [[1.4856043  0.15516312]]\n",
      "before\n",
      "after, [[0.05400088 0.00197005]]\n",
      "before\n",
      "after, [[0.05436644 0.00199092]]\n",
      "before\n",
      "after, [[1.3371111  0.20102136]]\n",
      "before\n",
      "after, [[1.336631   0.14165376]]\n",
      "before\n",
      "after, [[1.4857392  0.14584409]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "after, [[0.05833036 0.0022403 ]]\n",
      "before\n",
      "after, [[1.4820821 0.412204 ]]\n",
      "before\n",
      "after, [[1.3958795  0.19638671]]\n",
      "before\n",
      "after, [[0.06288363 0.00191629]]\n",
      "TIMESTEP 107 / EPSILON 0.094006 / ACTION 0 / REWARD -0.3235294117647059 / Q_MAX  0.06288363 / Loss  1.2867609793320298\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
